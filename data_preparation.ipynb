{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARATION AND MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1179055004553900032_twitter', '1179063826874032128_twitter', '1178793830532956161_twitter', '1179088797964763136_twitter', '1179085312976445440_twitter']\n",
      "{'post_id': '1179055004553900032_twitter', 'annotators': [{'label': 'normal', 'annotator_id': 1, 'target': ['None']}, {'label': 'normal', 'annotator_id': 2, 'target': ['None']}, {'label': 'normal', 'annotator_id': 3, 'target': ['None']}], 'rationales': [], 'post_tokens': ['i', 'dont', 'think', 'im', 'getting', 'my', 'baby', 'them', 'white', '9', 'he', 'has', 'two', 'white', 'j', 'and', 'nikes', 'not', 'even', 'touched']}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON file\n",
    "with open(\"dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Check sample structure\n",
    "print(list(data.keys())[:5])  # Print first 5 keys (post IDs)\n",
    "print(data[list(data.keys())[0]])  # Print first post details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text       label\n",
      "0  i dont think im getting my baby them white 9 h...      normal\n",
      "1  we cannot continue calling ourselves feminists...      normal\n",
      "2                      nawt yall niggers ignoring me      normal\n",
      "3  <user> i am bit confused coz chinese ppl can n...  hatespeech\n",
      "4  this bitch in whataburger eating a burger with...  hatespeech\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load the JSON file\n",
    "with open(\"dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract text and majority label\n",
    "texts, labels = [], []\n",
    "\n",
    "for post_id, post_data in data.items():\n",
    "    text = \" \".join(post_data[\"post_tokens\"])  # Convert tokens to sentence\n",
    "    \n",
    "    # Get all annotator labels\n",
    "    all_labels = [ann[\"label\"] for ann in post_data[\"annotators\"]]\n",
    "    \n",
    "    # Majority vote for final label\n",
    "    final_label = Counter(all_labels).most_common(1)[0][0]\n",
    "\n",
    "    texts.append(text)\n",
    "    labels.append(final_label)\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame({\"text\": texts, \"label\": labels})\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(\"hatexplain_processed.csv\", index=False)\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "normal        8153\n",
      "hatespeech    6234\n",
      "offensive     5761\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERSAMPLING THE MINOR DATATSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "normal        8153\n",
      "hatespeech    8153\n",
      "offensive     8153\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"hatexplain_processed.csv\")\n",
    "\n",
    "# Define oversampling strategy\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Apply oversampling\n",
    "X_resampled, y_resampled = ros.fit_resample(df[[\"text\"]], df[\"label\"])\n",
    "\n",
    "# Convert back to DataFrame\n",
    "df_balanced = pd.DataFrame({\"text\": X_resampled[\"text\"], \"label\": y_resampled})\n",
    "\n",
    "# Save the balanced dataset\n",
    "df_balanced.to_csv(\"hatexplain_balanced.csv\", index=False)\n",
    "\n",
    "# Check new class distribution\n",
    "print(df_balanced[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\priya\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels before conversion: ['normal' 'hatespeech' 'offensive']\n",
      "Unique labels after conversion: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"hatexplain_balanced.csv\")\n",
    "\n",
    "# Check unique labels\n",
    "print(\"Unique labels before conversion:\", df[\"label\"].unique())\n",
    "\n",
    "# Convert labels from text to numeric values\n",
    "label_mapping = {\"normal\": 0, \"hatespeech\": 1, \"offensive\": 2}\n",
    "df[\"label\"] = df[\"label\"].map(label_mapping)\n",
    "\n",
    "# Ensure labels are converted correctly\n",
    "print(\"Unique labels after conversion:\", df[\"label\"].unique())\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load DistilBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "print(\"âœ… Tokenizer loaded successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24459/24459 [00:08<00:00, 2961.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    encoding = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    encoding[\"labels\"] = batch[\"label\"]  # Ensure labels are included\n",
    "    return encoding\n",
    "\n",
    "# Apply tokenization\n",
    "dataset = dataset.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test[\"train\"]\n",
    "eval_dataset = train_test[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL : tinybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"huawei-noah/TinyBERT_General_6L_768D\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 classes: normal, hatespeech, offensive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "\n",
    "# Modify Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,  # Set a higher value; early stopping will stop it automatically\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,  # Ensures we save the best model\n",
    "    metric_for_best_model=\"eval_loss\",  # Stop when validation loss stops improving\n",
    "    greater_is_better=False,  # Since lower loss is better\n",
    ")\n",
    "\n",
    "# Define Trainer with Early Stopping\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # Stop after 2 epochs of no improvement\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4892' max='12230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4892/12230 41:34 < 1:02:23, 1.96 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.746600</td>\n",
       "      <td>0.776482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.776700</td>\n",
       "      <td>0.733485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.565200</td>\n",
       "      <td>0.752519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.489400</td>\n",
       "      <td>0.786026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/huawei-noah/TinyBERT_General_6L_768D/93343d2b799d2f2d29ef6c2c7ce01906d4fc47f58dbb908048c58c5d76a018b3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1742919950&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjkxOTk1MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9odWF3ZWktbm9haC9UaW55QkVSVF9HZW5lcmFsXzZMXzc2OEQvOTMzNDNkMmI3OTlkMmYyZDI5ZWY2YzJjN2NlMDE5MDZkNGZjNDdmNThkYmI5MDgwNDhjNThjNWQ3NmEwMThiMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=VAB9MpbDYdqpyq9MdHxEoqnyDe5P7A4%7EDpFNOACmE3JJI-9rElzIPQZhqoYS1vzrZyFJ5S6ZCxGTltokG25%7EylOuvO1f-7qPfY4HBZCGiY6IZcVvRQkbF7-QN2hxQmgjr-EtqaHfcL2Xb4zsnKQiS%7EuqxrD0NY5W--ORaPS4LR0F7S5QAgE6RW2cRXveFFr6PQ%7ERgwBWqYa47Afut6l2Px1CfaDfvfGWspmJR8VbcLZ58MoTaIh1Hl3C0XKwadcKzseLFABdfxbDLYwbbYGsuaYd8xC2jonTCSWS6VroAEAfkLj9h%7E88v5CLI3ssiOjAiXn2d5R6Ck5FTD%7EoBcWDcw__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training complete.\n",
      "âœ… Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸš€ Starting training...\")\n",
    "trainer.train()\n",
    "print(\"âœ… Training complete.\")\n",
    "model.save_pretrained(\"tinybert\")\n",
    "tokenizer.save_pretrained(\"tinybert_hate_speech\")\n",
    "print(\"âœ… Model and tokenizer saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "\n",
    "# âœ… Define Paths\n",
    "model_path = \"tinybert\"\n",
    "tokenizer_path = \"tinybert_hate_speech\"\n",
    "model_file = f\"{model_path}/model.safetensors\"\n",
    "\n",
    "# âœ… Load Config\n",
    "config = AutoConfig.from_pretrained(model_path)  \n",
    "\n",
    "# âœ… Load Model Architecture\n",
    "model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "# âœ… Load Weights from Safetensors\n",
    "state_dict = load_file(model_file)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# âœ… Load Tokenizer from Different Directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "print(\"âœ… Model and tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: hatespeech\n",
      "Probabilities: [[0.09392083436250687, 0.5404053330421448, 0.36567381024360657]]\n"
     ]
    }
   ],
   "source": [
    "def predict(text, model, tokenizer):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Move input tensors to model's device (CPU/GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Convert logits to probabilities (softmax)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Get predicted label\n",
    "    predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "    # Label mapping (modify if your dataset uses different labels)\n",
    "    label_map = {0: \"normal\", 1: \"hatespeech\", 2: \"offensive\"}\n",
    "    return label_map[predicted_class], probs.tolist()\n",
    "\n",
    "# Example usage\n",
    "text = \"fuck you!\"  # Change this for testing\n",
    "prediction, probabilities = predict(text, model, tokenizer)\n",
    "\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Probabilities: {probabilities}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
